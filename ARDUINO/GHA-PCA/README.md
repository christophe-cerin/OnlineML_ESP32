## Generalized Hebbian  Algorithm  (GHA)

The Generalized Hebbian Algorithm is a neural approach to principal component analysis (PCA) developed by Sanger, known as the Generalized Hebbian Algorithm (GHA). This algorithm is mainly used in the field of image processing, in particular for image compression while preserving maximum information (optimal coding). The GHA is able to extract the principal components of very high-dimensional vectors, characteristic of the data, which corresponds to the dimension of the representation. Moreover, it focuses only on the calculation of the first, most significant, principal components, which results in a considerable reduction in the required computational resources. Thanks to its adaptive nature, the algorithm can also provide a rough estimate of the results, which can be refined later if necessary, unlike classical PCA software that calculates all eigenvectors with maximum precision, even if this is not always required.

However, GHA has some drawbacks. As is often the case with neural techniques, the learning step must be estimated empirically, and learning times can be long if we are looking for good precision, especially for the following components with lower variance. And this algorithm causes calculation errors to accumulate from one neuron to the next, which inevitably reduces the precision of the successive components. It is therefore important in practice to limit ourselves to the first principal components. It should be remembered that the algorithm does not directly give the share of variance corresponding to each principal component. We can then easily calculate the output variance of each neuron on a sample of the corpus in order to find the number of components that are actually useful (we will stop learning new neurons when their variance becomes insufficient).

In short, AHG is only justified when we can be satisfied with the first principal components. But this is often the case, since they represent the largest part of the information contained in the data. Finally, like PCA, HGA is a purely linear method that can only capture linear correlations between data (this is actually equivalent to using only covariance).

