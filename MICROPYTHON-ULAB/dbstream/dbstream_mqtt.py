from __future__ import annotations

#from river import metrics, stream, utils
#from river.cluster import DBSTREAM
import collections
#import functools
import copy
import math

# Cerin: initial code next line
#from river import base
from clusterer import *



class DBSTREAM():
    r"""DBSTREAM

    DBSTREAM [^1] is a clustering algorithm for evolving data streams.
    It is the first micro-cluster-based online clustering component that
    explicitely captures the density between micro-clusters via a shared
    density graph. The density information in the graph is then exploited
    for reclustering based on actual density between adjacent micro clusters.

    The algorithm is divided into two parts:

    **Online micro-cluster maintenance (learning)**

    For a new point `p`:

    * Find all micro clusters for which `p` falls within the fixed radius
    (clustering threshold). If no neighbor is found, a new micro cluster
    with a weight of 1 is created for `p`.

    * If no neighbor is found, a new micro cluster with a weight of 1 is
    created for `p`. If one or more neighbors of `p` are found, we update
    the micro clusters by applying the appropriate fading, increasing
    their weight and then we try to move them closer to `p` using the
    Gaussian neighborhood function.

    * Next, the shared density graph is updated. To prevent collapsing
    micro clusters, we will restrict the movement for micro clusters in case
    they come closer than $r$ (clustering threshold) to each other. Finishing
    this process, the time stamp is also increased by 1.

    * Finally, the cleanup will be processed. It is executed every `t_gap`
    time steps, removing weak micro clusters and weak entries in the
    shared density graph to recover memory and improve the clustering algorithm's
    processing speed.

    **Offline generation of macro clusters (clustering)**

    The offline generation of macro clusters is generated through the two following steps:

    * The connectivity graph `C` is constructed using shared density entries
    between strong micro clusters. The edges in this connectivity graph with
    a connectivity value greater than the intersection threshold ($\alpha$)
    are used to find connected components representing the final cluster.

    * After the connectivity graph is generated, a variant of the DBSCAN algorithm
    proposed by Ester et al. is applied to form all macro clusters
    from $\alpha$-connected micro clusters.

    Parameters
    ----------
    clustering_threshold
        DBStream represents each micro cluster by a leader (a data point defining the
        micro cluster's center) and the density in an area of a user-specified radius
        $r$ (`clustering_threshold`) around the center.
    fading_factor
        Parameter that controls the importance of historical data to current cluster.
        Note that `fading_factor` has to be different from `0`.
    cleanup_interval
        The time interval between two consecutive time points when the cleanup process is
         conducted.
    minimum_weight
        The minimum weight for a cluster to be not "noisy".
    intersection_factor
        The intersection factor related to the area of the overlap of the micro clusters
        relative to the area cover by micro clusters. This parameter is used to determine
        whether a micro cluster or a shared density is weak.

    Attributes
    ----------
    n_clusters
        Number of clusters generated by the algorithm.
    clusters
        A set of final clusters of type `DBStreamMicroCluster`. However, these are either
        micro clusters, or macro clusters that are generated by merging all $\alpha$-connected
        micro clusters. This set is generated through the offline phase of the algorithm.
    centers
        Final clusters' centers.
    micro_clusters
        Micro clusters generated by the algorithm. Instead of updating directly the new instance points
        into a nearest micro cluster, through each iteration, the weight and center will be modified
        so that the clusters are closer to the new points, using the Gaussian neighborhood function.

    References
    ----------
    [^1]: Michael Hahsler and Matthew Bolanos (2016, pp 1449-1461). Clustering Data Streams Based on
          Shared Density between Micro-Clusters, IEEE Transactions on Knowledge and Data Engineering 28(6) .
          In Proceedings of the Sixth SIAM International Conference on Data Mining,
          April 20â€“22, 2006, Bethesda, MD, USA.
    [^2]: Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
          with Noise. In KDD-96 Proceedings, AAAI.

    Examples
    --------

    >>> from river import cluster
    >>> from river import stream

    >>> X = [
    ...     [1, 0.5], [1, 0.625], [1, 0.75], [1, 1.125], [1, 1.5], [1, 1.75],
    ...     [4, 1.5], [4, 2.25], [4, 2.5], [4, 3], [4, 3.25], [4, 3.5]
    ... ]

    >>> dbstream = cluster.DBSTREAM(
    ...     clustering_threshold=1.5,
    ...     fading_factor=0.05,
    ...     cleanup_interval=4,
    ...     intersection_factor=0.5,
    ...     minimum_weight=1
    ... )

    >>> for x, _ in stream.iter_array(X):
    ...     dbstream.learn_one(x)

    >>> dbstream.predict_one({0: 1, 1: 2})
    0

    >>> dbstream.predict_one({0: 5, 1: 2})
    1

    >>> dbstream._n_clusters
    2

    """

    def __init__(
        self,
        clustering_threshold: float = 1.0,
        fading_factor: float = 0.01,
        cleanup_interval: float = 2,
        intersection_factor: float = 0.3,
        minimum_weight: float = 1.0,
    ):
        super().__init__()
        self._time_stamp = 0

        self.clustering_threshold = clustering_threshold
        self.fading_factor = fading_factor
        self.cleanup_interval = cleanup_interval
        self.intersection_factor = intersection_factor
        self.minimum_weight = minimum_weight

        self._n_clusters: int = 0
        self._clusters: dict[int, DBSTREAMMicroCluster] = {}
        self._centers: dict = {}
        self._micro_clusters: dict[int, DBSTREAMMicroCluster] = {}

        self.s: dict[int, dict[int, float]] = {}
        self.s_t: dict[int, dict[int, float]] = {}

        self.clustering_is_up_to_date = False

    @staticmethod
    def _distance(point_a, point_b):
        #return utils.math.minkowski_distance(point_a, point_b, 2) #Kayani Comment with below code without using Utils
        # Ensure the points have the same dimensions
        distance = 0
        p=2
        for key in point_a:
            distance += abs(point_a[key] - point_b[key]) ** p
        return distance ** (1 / p)
        

    def _find_fixed_radius_nn(self, x):
        fixed_radius_nn = {}
        for i in self._micro_clusters.keys():
            if self._distance(self._micro_clusters[i].center, x) < self.clustering_threshold:
                fixed_radius_nn[i] = self._micro_clusters[i]
        return fixed_radius_nn

    def _gaussian_neighborhood(self, point_a, point_b):
        distance = self._distance(point_a, point_b)
        sigma = self.clustering_threshold / 3
        gaussian_neighborhood = math.exp(-(distance * distance) / (2 * (sigma * sigma)))
        return gaussian_neighborhood

    def _update(self, x):
        # Algorithm 1 of Michael Hahsler and Matthew Bolanos
        neighbor_clusters = self._find_fixed_radius_nn(x)

        if len(neighbor_clusters) < 1:
            # create new micro cluster
            if len(self._micro_clusters) > 0:
                self._micro_clusters[max(self._micro_clusters.keys()) + 1] = DBSTREAMMicroCluster(
                    x=x, last_update=self._time_stamp, weight=1
                )
            else:
                self._micro_clusters[0] = DBSTREAMMicroCluster(
                    x=x, last_update=self._time_stamp, weight=1
                )
        else:
            # update existing micro clusters
            current_centers = {}
            for i in neighbor_clusters.keys():
                current_centers[i] = self._micro_clusters[i].center
                self._micro_clusters[i].weight = (
                    self._micro_clusters[i].weight
                    * 2
                    ** (
                        -self.fading_factor
                        * (self._time_stamp - self._micro_clusters[i].last_update)
                    )
                    + 1
                )

                # Update the center (i) with overlapping keys (j)
                self._micro_clusters[i].center = {
                    j: self._micro_clusters[i].center[j]
                    + self._gaussian_neighborhood(x, self._micro_clusters[i].center)
                    * (x[j] - self._micro_clusters[i].center[j])
                    for j in self._micro_clusters[i].center.keys()
                    if j in x
                }
                self._micro_clusters[i].last_update = self._time_stamp

                # update shared density
                for j in neighbor_clusters.keys():
                    if j > i:
                        try:
                            self.s[i][j] = (
                                self.s[i][j]
                                * 2 ** (-self.fading_factor * (self._time_stamp - self.s_t[i][j]))
                                + 1
                            )
                            self.s_t[i][j] = self._time_stamp
                        except KeyError:
                            try:
                                self.s[i][j] = 1
                                self.s_t[i][j] = self._time_stamp
                            except KeyError:
                                self.s[i] = {j: 1}
                                self.s_t[i] = {j: self._time_stamp}

            # prevent collapsing clusters
            for i in neighbor_clusters.keys():
                for j in neighbor_clusters.keys():
                    if j > i:
                        if (
                            self._distance(
                                self._micro_clusters[i].center,
                                self._micro_clusters[j].center,
                            )
                            < self.clustering_threshold
                        ):
                            # revert centers of mc_i and mc_j to previous positions
                            self._micro_clusters[i].center = current_centers[i]
                            self._micro_clusters[j].center = current_centers[j]

        self._time_stamp += 1

    def _cleanup(self):
        # Algorithm 2 of Michael Hahsler and Matthew Bolanos: Cleanup process to remove
        # inactive clusters and shared density entries from memory
        weight_weak = 2 ** (-self.fading_factor * self.cleanup_interval)

        micro_clusters = copy.deepcopy(self._micro_clusters)
        for i, micro_cluster_i in self._micro_clusters.items():
            try:
                value = 2 ** (
                    -self.fading_factor * (self._time_stamp - micro_cluster_i.last_update)
                )
            except OverflowError:
                continue

            if micro_cluster_i.weight * value < weight_weak:
                micro_clusters.pop(i)
                self.s.pop(i, None)
                self.s_t.pop(i, None)
                # Since self.s and self.s_t always have the same keys and are arranged in ascending orders
                for j in self.s:
                    if j < i:
                        self.s[j].pop(i, None)
                        self.s_t[j].pop(i, None)
                    else:
                        break

        # Update microclusters
        self._micro_clusters = micro_clusters

        for i in self.s.keys():
            for j in self.s[i].keys():
                try:
                    value = 2 ** (-self.fading_factor * (self._time_stamp - self.s_t[i][j]))
                except OverflowError:
                    continue

                if self.s[i][j] * value < self.intersection_factor * weight_weak:
                    self.s[i][j] = 0
                    self.s_t[i][j] = 0

    def _generate_weighted_adjacency_matrix(self):
        # Algorithm 3 of Michael Hahsler and Matthew Bolanos: Reclustering using
        # shared density graph
        weighted_adjacency_matrix = {}
        for i in list(self.s.keys()):
            for j in list(self.s[i].keys()):
                try:
                    if (
                        self._micro_clusters[i].weight <= self.minimum_weight
                        or self._micro_clusters[j].weight <= self.minimum_weight
                    ):
                        continue
                except KeyError:
                    continue

                value = self.s[i][j] / (
                    (self._micro_clusters[i].weight + self._micro_clusters[j].weight) / 2
                )
                if value > self.intersection_factor:
                    try:
                        weighted_adjacency_matrix[i][j] = value
                    except KeyError:
                        weighted_adjacency_matrix[i] = {j: value}

        return weighted_adjacency_matrix

    """def _generate_labels(self, weighted_adjacency_list):
        # This function handles the weighted adjacency list created above and
        # generate a cluster label for all micro clusters, using a variant of
        # the DBSCAN algorithm proposed by Ester et al. for alpha-connected micro clusters

        # initiate labels of micro clusters to None
        labels = {i: None for i in self._micro_clusters.keys()}

        # cluster counter; in this algorithm, cluster labels starts with 0
        count = -1

        for index in labels.keys():
            if labels[index] is not None:
                continue
            count += 1
            labels[index] = count
            # if it is not in list of alpha-connected micro-clusters, label and continue
            if index not in weighted_adjacency_list.keys():
                continue
            
            # Get the keys of the dictionary at weighted_adjacency_list[index]
            keys = list(weighted_adjacency_list[index].keys())
            # Create a deque with the keys
            seed_set = collections.deque(keys)
            #seed_set = collections.deque(weighted_adjacency_list[index].keys())
            while seed_set:
                # check previously processed points
                if labels[seed_set[0]] is not None:
                    seed_set.popleft()
                    continue
                # proceed DBSCAN when seed set is not blank
                if seed_set:
                    labels[seed_set[0]] = count
                    # find neighbors
                    if seed_set[0] in weighted_adjacency_list.keys():
                        neighbor_neighbors = collections.deque(
                            weighted_adjacency_list[seed_set[0]].keys()
                        )
                        # add new neighbors to seed set
                        for neighbor_neighbor in neighbor_neighbors:
                            if labels[neighbor_neighbor] is None:
                                seed_set.append(neighbor_neighbor)

        return labels"""
    
    def _generate_labels(self, weighted_adjacency_list):
        # This function handles the weighted adjacency list created above and
        # generate a cluster label for all micro clusters, using a variant of
        # the DBSCAN algorithm proposed by Ester et al. for alpha-connected micro clusters

        # initiate labels of micro clusters to None
        labels = {i: None for i in self._micro_clusters.keys()}

        # cluster counter; in this algorithm, cluster labels starts with 0
        count = -1

        for index in labels.keys():
            if labels[index] is not None:
                continue
            count += 1
            labels[index] = count
            # if it is not in list of alpha-connected micro-clusters, label and continue
            if index not in weighted_adjacency_list.keys():
                continue
            
            # Get the keys of the dictionary at weighted_adjacency_list[index]
            keys = list(weighted_adjacency_list[index].keys())
            # Create a list with the keys
            seed_set = list(keys)
            while seed_set:
                # check previously processed points
                if labels[seed_set[0]] is not None:
                    seed_set.pop(0)
                    continue
                # proceed DBSCAN when seed set is not blank
                if seed_set:
                    labels[seed_set[0]] = count
                    # find neighbors
                    if seed_set[0] in weighted_adjacency_list.keys():
                        neighbor_neighbors = list(weighted_adjacency_list[seed_set[0]].keys())
                        # add new neighbors to seed set
                        for neighbor_neighbor in neighbor_neighbors:
                            if labels[neighbor_neighbor] is None:
                                seed_set.append(neighbor_neighbor)
                    seed_set.pop(0)  # Move this line to ensure the current element is removed after processing

        return labels


    def _generate_clusters_from_labels(self, cluster_labels):
        # initiate the set for final clusters
        clusters = {}

        # generate set of clusters with the same label with the structure {j: micro_cluster_index}
        for i in range(max(cluster_labels.values()) + 1):
            j = 0
            mcs_with_label_i = {}
            for index, label in cluster_labels.items():
                if label == i:
                    mcs_with_label_i[j] = self._micro_clusters[index]
                    j += 1

            # generate a final macro-cluster from clusters with the same label using the
            # merge function of DBStreamMicroCluster
            macro_cluster = copy.deepcopy(mcs_with_label_i[0])
            for m in range(1, len(mcs_with_label_i)):
                macro_cluster.merge(mcs_with_label_i[m])

            clusters[i] = macro_cluster

        n_clusters = len(clusters)

        return n_clusters, clusters

    def _recluster(self):
        # Algorithm 3 of Michael Hahsler and Matthew Bolanos: Reclustering
        # using shared density graph
        if self.clustering_is_up_to_date:
            return

        weighted_adjacency_list = self._generate_weighted_adjacency_matrix()

        labels = self._generate_labels(weighted_adjacency_list)

        # We can only update given we have labels (possibly not on first pass)
        if labels:
            self._n_clusters, self._clusters = self._generate_clusters_from_labels(labels)
            self._centers = {i: self._clusters[i].center for i in self._clusters.keys()}

        self.clustering_is_up_to_date = True

    def learn_one(self, x, w=None):
        self._update(x)

        if self._time_stamp % self.cleanup_interval == 0:
            self._cleanup()

        self.clustering_is_up_to_date = False

    def predict_one(self, x, w=None):
        self._recluster()

        min_distance = math.inf

        # default result of all clustering results, regardless of whether there already
        # exists macro-clusters
        closest_cluster_index = 0

        for i, center_i in self._centers.items():
            distance = self._distance(center_i, x)
            if distance < min_distance:
                min_distance = distance
                closest_cluster_index = i
        return closest_cluster_index

    @property
    def n_clusters(self) -> int:
        self._recluster()
        return self._n_clusters

    @property
    def clusters(self) -> dict[int, DBSTREAMMicroCluster]:
        self._recluster()
        return self._clusters

    @property
    def centers(self) -> dict:
        self._recluster()
        return self._centers

    @property
    def micro_clusters(self) -> dict[int, DBSTREAMMicroCluster]:
        return self._micro_clusters


class DBSTREAMMicroCluster():
    """DBStream Micro-cluster class"""

    def __init__(self, x=None, last_update=None, weight=None):
        self.center = x
        self.last_update = last_update
        self.weight = weight

    def merge(self, cluster):
        # Using cluster.center.get allows updating clusters with different features
        self.center = {
            i: (self.center[i] * self.weight + cluster.center.get(i, 0.0) * cluster.weight)
            / (self.weight + cluster.weight)
            for i in self.center.keys()
        }
        self.weight += cluster.weight

# SELF Onward

"""X =[
    [1.25, 1.5], [1.25, 1.6], [1.25, 1.7], [1.0, 1.5], [1.1, 1.625],
    [1.2, 1.75], [1.0, 1.125], [1.1, 1.5], [1.2, 1.75],
    [4.0, 4.5], [4.1,4.25], [4.2, 4.5], [4.0, 4], [4.1, 4.25],
    [4.2, 4.5], [1.25, 1.5], [1.25, 1.6], [1.25, 1.7]
] 
X =[[264.0, 20.0],[269.0, 21.7],[262.0, 20.1],[266.0, 21.6],[252.0, 20.0],[276.0, 21.6],[259.0, 20.0],[270.0, 21.6],[261.0, 20.0],[264.0, 21.6],[261.0, 21.6],[266.0, 20.0],[262.0, 21.6],[256.0, 20.0],[263.0, 21.5],[250.0, 20.0],[253.0, 20.0],[256.0, 21.6],[267.0, 20.0],[276.0, 21.6],[263.0, 20.0],[268.0, 20.0],[269.0, 21.5],[251.0, 20.1],[281.0, 21.6],[273.0, 21.7],[257.0, 20.3],[277.0, 21.8],[257.0, 20.4],[262.0, 21.8],[260.0, 20.3],[265.0, 21.9],[259.0, 20.4],[278.0, 21.9],[257.0, 20.4],[271.0, 21.9],[256.0, 20.5],[258.0, 20.5],[268.0, 22.0],[268.0, 20.5],[277.0, 22.1],[249.0, 20.5],[267.0, 22.1],[253.0, 20.5],[268.0, 22.1],[259.0, 20.5],[266.0, 22.1],[267.0, 22.1],[266.0, 20.5],[282.0, 22.2],[258.0, 20.6],[271.0, 22.2],[258.0, 20.5],[274.0, 22.1],[258.0, 20.6],[269.0, 22.2],[265.0, 20.6],[275.0, 22.2],[255.0, 20.6],[266.0, 22.2],[252.0, 20.6],[268.0, 22.2],[256.0, 20.5],[277.0, 22.2],[263.0, 20.6],[281.0, 22.2],[263.0, 20.6],[267.0, 22.3],[253.0, 20.7],[271.0, 22.3],[258.0, 20.7],[272.0, 22.3],[261.0, 20.7],[275.0, 22.3],[263.0, 22.4],[260.0, 20.7],[276.0, 22.4],[260.0, 20.8],[273.0, 22.5],[264.0, 20.7],[283.0, 22.5],[263.0, 20.7],[266.0, 22.5],[265.0, 20.7],[265.0, 22.4],[278.0, 22.5],[264.0, 22.6],[264.0, 22.7],[263.0, 22.7],[254.0, 20.8],[267.0, 22.7],[273.0, 22.7],[266.0, 22.8],[266.0, 22.7],[266.0, 20.9],[255.0, 22.7],[253.0, 21.0],[268.0, 22.7],[267.0, 21.0],[270.0, 22.7],[279.0, 22.7],[267.0, 21.0],[271.0, 22.7],[252.0, 20.9],[262.0, 22.6],[261.0, 22.7],[278.0, 22.6],[280.0, 22.6],[267.0, 22.6],[272.0, 22.6],[258.0, 20.9],[277.0, 22.6],[274.0, 22.5],[276.0, 22.5],[267.0, 20.8],[262.0, 22.4],[257.0, 20.8],[277.0, 22.4],[268.0, 22.3],[270.0, 22.3],[267.0, 20.7],[274.0, 22.2],[267.0, 20.7],[279.0, 22.2],[260.0, 20.6],[271.0, 22.2],[260.0, 20.6],[271.0, 22.2],[257.0, 20.5],[262.0, 20.6],[267.0, 22.1],[263.0, 20.6],[261.0, 22.1],[260.0, 20.6],[273.0, 22.2],[271.0, 22.2],[271.0, 20.6],[279.0, 22.1],[258.0, 20.5],[266.0, 22.1],[276.0, 22.1],[261.0, 20.5],[256.0, 20.5],[268.0, 22.1],[252.0, 20.5],[269.0, 22.1],[256.0, 20.5],[273.0, 22.1],[265.0, 20.5],[273.0, 22.0],[261.0, 20.4],[274.0, 22.1],[263.0, 20.5],[277.0, 22.1],[256.0, 20.5],[258.0, 22.1],[268.0, 20.4],[265.0, 22.1],[261.0, 20.4],[289.0, 22.1],[259.0, 20.4],[285.0, 22.1],[280.0, 22.1],[259.0, 20.4],[268.0, 22.1],[259.0, 20.4],[281.0, 22.1],[256.0, 20.4],[266.0, 22.1],[257.0, 20.5],[277.0, 22.2],[262.0, 20.5],[275.0, 22.2],[258.0, 20.5],[283.0, 22.2],[269.0, 20.4],[281.0, 22.2],[257.0, 20.4],[269.0, 22.2],[250.0, 20.4],[269.0, 22.2],[257.0, 20.5],[275.0, 22.2],[261.0, 20.5],[274.0, 22.2],[258.0, 20.5],[273.0, 22.2],[270.0, 20.5],[268.0, 22.2],[277.0, 22.2],[264.0, 20.4],[272.0, 22.2],[269.0, 20.3],[266.0, 22.2],[281.0, 22.2],[257.0, 20.3],[275.0, 22.1],[245.0, 20.3],[279.0, 22.1],[275.0, 20.3],[273.0, 22.1],[255.0, 20.3],[268.0, 22.1],[267.0, 20.3],[261.0, 22.1],[261.0, 20.3],[281.0, 22.1],[265.0, 20.3],[284.0, 22.1],[285.0, 22.1],[260.0, 20.3],[256.0, 20.4],[276.0, 22.1],[274.0, 22.1],[257.0, 20.4],[250.0, 22.2],[268.0, 22.2],[263.0, 20.4],[262.0, 22.2],[281.0, 22.2],[271.0, 20.4],[285.0, 22.3],[253.0, 20.4],[279.0, 22.3],[281.0, 20.4],[274.0, 22.3],[257.0, 20.4],[270.0, 22.3],[261.0, 22.3],[272.0, 20.4],[272.0, 22.3],[252.0, 20.4],[263.0, 22.3],[271.0, 22.3],[258.0, 20.4],[275.0, 22.4],[267.0, 20.4],[269.0, 22.4],[277.0, 22.4],[265.0, 20.4],[266.0, 20.4],[263.0, 22.4],[276.0, 22.4],[267.0, 22.3],[269.0, 20.4],[273.0, 22.3],[258.0, 20.4],[271.0, 22.4],[264.0, 20.3],[284.0, 22.4],[277.0, 20.4],[268.0, 22.4],[259.0, 20.4],[264.0, 20.4],[265.0, 22.3],[259.0, 20.4],[274.0, 22.3],[267.0, 22.3],[271.0, 20.4],[277.0, 22.3],[280.0, 22.3],[251.0, 22.3],[285.0, 22.3],[266.0, 20.4],[269.0, 22.3],[278.0, 22.3],[275.0, 20.4],[267.0, 22.3],[259.0, 20.4],[257.0, 22.2],[269.0, 20.4],[282.0, 22.3],[272.0, 22.2],[271.0, 22.3],[279.0, 22.2],[273.0, 22.3],[267.0, 20.3],[265.0, 22.2],[261.0, 22.2],[278.0, 20.3],[258.0, 20.2],[263.0, 22.2],[264.0, 20.2],[265.0, 22.2],[264.0, 20.3],[278.0, 22.2],[273.0, 20.2],[275.0, 22.2],[250.0, 20.2],[272.0, 22.1],[290.0, 22.1],[265.0, 20.2],[261.0, 22.1],[266.0, 20.2],[270.0, 22.2],[265.0, 20.2],[266.0, 22.1],[255.0, 20.2],[261.0, 22.1],[269.0, 20.2],[273.0, 22.1],[268.0, 20.2],[271.0, 22.1],[281.0, 20.2],[267.0, 22.1],[273.0, 20.1],[281.0, 22.1],[269.0, 20.1],[281.0, 22.2],[265.0, 20.1],[257.0, 22.1],[262.0, 20.1],[273.0, 22.1],[264.0, 20.1],[276.0, 20.1],[279.0, 22.1],[264.0, 20.1],[278.0, 22.2],[272.0, 20.1],[271.0, 22.1],[267.0, 20.1],[276.0, 22.1],[257.0, 20.1],[264.0, 22.1],[270.0, 20.1],[247.0, 22.2],[264.0, 20.1],[280.0, 22.1],[276.0, 20.1],[274.0, 22.2],[281.0, 22.1],[271.0, 20.1],[267.0, 22.2],[283.0, 20.1],[271.0, 22.1],[274.0, 22.1],[258.0, 20.1],[265.0, 22.1],[271.0, 22.2],[268.0, 20.1],[269.0, 22.1],[267.0, 22.2],[272.0, 20.1],[271.0, 22.2],[280.0, 22.1],[268.0, 20.0],[275.0, 22.2],[272.0, 22.2],[277.0, 22.1],[270.0, 20.0],[273.0, 22.1],[285.0, 19.9],[264.0, 22.2],[268.0, 20.0],[273.0, 22.1],[261.0, 22.1],[273.0, 19.9],[276.0, 22.1],[281.0, 22.1],[279.0, 20.0],[271.0, 22.2],[268.0, 20.0],[273.0, 22.2],[271.0, 20.1],[270.0, 22.2],[279.0, 20.0],[266.0, 22.2],[278.0, 22.2],[265.0, 22.1],[279.0, 20.0],[276.0, 22.2],[274.0, 20.1],[271.0, 22.2],[274.0, 20.0],[268.0, 22.3],[272.0, 20.1],[270.0, 22.2],[271.0, 22.3],[278.0, 20.0],[270.0, 22.4],[264.0, 20.1],[263.0, 20.1],[258.0, 22.4],[271.0, 20.1],[260.0, 20.1],[259.0, 22.4],[259.0, 22.5],[272.0, 20.2],[267.0, 22.5],[274.0, 22.5],[269.0, 22.5],[275.0, 22.6],[270.0, 22.7],[280.0, 20.2],[262.0, 22.6],[274.0, 20.3],[272.0, 22.6],[263.0, 20.2],[258.0, 22.6],[275.0, 20.3],[262.0, 22.6],[268.0, 20.2],[272.0, 22.6],[262.0, 22.6],[277.0, 22.6],[264.0, 20.3],[281.0, 22.6],[273.0, 22.6],[277.0, 22.6],[263.0, 20.3],[267.0, 20.3],[292.0, 22.5],[254.0, 20.3],[269.0, 22.5],[273.0, 20.3],[265.0, 22.6],[274.0, 20.3],[272.0, 22.5],[268.0, 20.4],[277.0, 22.5],[272.0, 20.4],[276.0, 22.5],[267.0, 20.4],[289.0, 22.5],[267.0, 20.4],[274.0, 22.5],[272.0, 20.4],[270.0, 20.4],[270.0, 22.5],[276.0, 20.4],[278.0, 22.5],[277.0, 20.4],[280.0, 22.5],[275.0, 20.4],[268.0, 22.5],[265.0, 20.4],[278.0, 22.5],[266.0, 20.4],[272.0, 22.5],[264.0, 20.4],[286.0, 22.5],[275.0, 20.3],[278.0, 22.5],[269.0, 20.3],[262.0, 22.5],[274.0, 20.3],[266.0, 22.5],[270.0, 20.3],[289.0, 22.5],[281.0, 20.4],[279.0, 22.5],[258.0, 20.3],[283.0, 22.5],[269.0, 20.4],[265.0, 22.5],[282.0, 20.4],[275.0, 22.5],[270.0, 22.5],[285.0, 22.5],[274.0, 22.5],[263.0, 20.4],[275.0, 22.5],[267.0, 20.3],[298.0, 22.5],[266.0, 20.3],[278.0, 22.5],[277.0, 22.5],[261.0, 20.3],[273.0, 22.5],[270.0, 20.3],[282.0, 22.5],[277.0, 20.3],[274.0, 22.5],[267.0, 20.3],[284.0, 22.5],[267.0, 20.3],[294.0, 22.5],[267.0, 20.3],[291.0, 22.5],[286.0, 22.5],[279.0, 20.3],[275.0, 22.5],[267.0, 20.3],[279.0, 22.5],[282.0, 22.5],[283.0, 20.3],[299.0, 22.5],[286.0, 20.3],[287.0, 20.2],[285.0, 22.5],[277.0, 20.3],[284.0, 22.5],[264.0, 20.3],[266.0, 22.5],[277.0, 20.3],[284.0, 22.5],[285.0, 20.3],[286.0, 22.5],[283.0, 20.3],[286.0, 22.5],[267.0, 20.3],[289.0, 22.5],[265.0, 20.3],[278.0, 22.5],[277.0, 20.3],[270.0, 20.3],[284.0, 22.5],[283.0, 22.5],[261.0, 20.3],[274.0, 22.5],[268.0, 20.2],[286.0, 22.4]]

def build_dbstream(fading_factor=0.01, intersection_factor=0.005):
    return DBSTREAM(
        fading_factor=fading_factor,
        clustering_threshold=6,  # Adjusted to create 2 clusters
        cleanup_interval=2,
        intersection_factor=intersection_factor,
        minimum_weight=1
    )

X_dicts = [dict(enumerate(x)) for x in X]
import time

# Create an instance of DBstream
dbstream = build_dbstream()

# Train the model and print the assigned cluster for each data point
for i, x in enumerate(X_dicts):
    dbstream.learn_one(x)
    
    # print(" Cluster centers:")
    # k_means.print_centers()
    print(f'[{X[i]}, {dbstream.predict_one(x)}],')
    # Pause for 1 seconds
    #time.sleep(1)
    #print(f'{X[i]} is assigned to cluster {k_means.predict_one(x)}')

# print(" Cluster centers:")
# dbstream.print_centers()
"""

#-------- Getting MQTT Data --------
# Connection of MqTT to collect real time data
import network
from umqtt.simple import MQTTClient

# WiFi connection details
WIFI_SSID = 'wifi-test'
WIFI_PASSWORD = 'Sunny789'

# MQTT broker details
MQTT_BROKER = '130.190.74.222'  # Change this to the IP address or hostname of your MQTT broker
MQTT_PORT = 1883
MQTT_USER = 'root'
MQTT_PASSWORD = 'Sunny789'
MQTT_TOPIC = 'test/topic'

def connect_wifi():
    sta_if = network.WLAN(network.STA_IF)
    if not sta_if.isconnected():
        print('Connecting to WiFi...')
        sta_if.active(True)
        sta_if.connect(WIFI_SSID, WIFI_PASSWORD)
        while not sta_if.isconnected():
            pass
    print('WiFi connected:', sta_if.ifconfig())

def safe_float_conversion(value):
    try:
        return  float(value.strip())
    except ValueError:
        return 0  # or handle it in a way that suits your needs



def build_dbstream(fading_factor=0.01, intersection_factor=0.005):
    return DBSTREAM(
        fading_factor=fading_factor,
        clustering_threshold=6,  # Adjusted to create 2 clusters
        cleanup_interval=2.2,
        intersection_factor=intersection_factor,
        minimum_weight=1
    )
#String Output Cluster Result
incoming_stream=[]
dbstream = build_dbstream()
def on_message(topic, msg):    
    data = msg.decode("utf-8").split(",")
    iteration=1
    temperature_data = []
    
    for item in data:
        temperature_data.append(safe_float_conversion(item))
        
    #Check FOR FIRST redundant row 
    if(temperature_data[0]==0 and temperature_data[1]==0):
        return
    incoming_stream.append(temperature_data)
    
    
    if((len(incoming_stream)==128)):
        #print("my enum:", my_enum)
        for iter in range(iteration):
            #print(iter,"th Iteration !!!!")
            my_enum = enumerate(iter_array.iter_array(incoming_stream))
            for i, (x,_) in my_enum:
                #print("i :", i, "  x : ", x, " _  : ", _ )
                #print('Before learn_one:',x,' Type:',type(x))
                dbstream.learn_one(x)
                cluster= dbstream.predict_one(x)
                #print(f'[{i}] ==> {incoming_stream[i]} is assigned to cluster {cluster}')
                if iter== (iteration-1):
                    print(f'[{incoming_stream[i]}, {cluster}],')
                #Storing clucter result
                #cluster_output.append([X[i], cluster])
                #print("Clusters results : ")
        incoming_stream.clear()      

#-------- Getting MQTT Data End --------
#
# Main
#         
import iter_array
from ulab import numpy as np

connect_wifi()
client = MQTTClient("esp32", MQTT_BROKER, MQTT_PORT, MQTT_USER, MQTT_PASSWORD)
client.set_callback(on_message)
client.connect()
client.subscribe(MQTT_TOPIC)
print('Connected to MQTT broker')

try:
    while True:
        client.wait_msg()
finally:
    client.disconnect()
    print('Disconnected from MQTT broker')




